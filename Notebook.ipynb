{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import parse_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_posts = 'data/Posts.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = parse_xml.posts_to_df(path_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the columns we need\n",
    "\n",
    "posts_df = posts_df[['Id', 'PostTypeId', 'ParentId', 'Body', 'AcceptedAnswerId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = posts_df[posts_df['PostTypeId'] == 1]\n",
    "answers_df = posts_df[posts_df['PostTypeId'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erjon\\AppData\\Local\\Temp\\ipykernel_17772\\855821916.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions_df['Body'] = questions_df.loc[:, 'Body'].str.replace(r'<[^>]*>', '', regex=True)\n",
      "C:\\Users\\Erjon\\AppData\\Local\\Temp\\ipykernel_17772\\855821916.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  answers_df['Body'] = answers_df.loc[:, 'Body'].str.replace(r'<[^>]*>', '', regex=True)\n"
     ]
    }
   ],
   "source": [
    "# remove all tags from the body of the posts using regex\n",
    "\n",
    "questions_df['Body'] = questions_df.loc[:, 'Body'].str.replace(r'<[^>]*>', '', regex=True)\n",
    "answers_df['Body'] = answers_df.loc[:, 'Body'].str.replace(r'<[^>]*>', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12014 12014\n"
     ]
    }
   ],
   "source": [
    "# remove the questions that have accepted answer id not 0\n",
    "\n",
    "questions_df = questions_df[questions_df['AcceptedAnswerId'] != 0].set_index('Id')\n",
    "answers_df = answers_df[answers_df['Id'].isin(questions_df['AcceptedAnswerId'])].set_index('Id')\n",
    "print(questions_df.shape[0], answers_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Id the index of the dataframes\n",
    "questions_df = questions_df.drop(columns=['PostTypeId', 'ParentId'])\n",
    "answers_df = answers_df.drop(columns=['PostTypeId', 'AcceptedAnswerId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframes to csv files\n",
    "\n",
    "questions_df.to_csv('data/questions.csv')\n",
    "answers_df.to_csv('data/answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataframes from the csv files\n",
    "\n",
    "questions_df = pd.read_csv('data/questions.csv', index_col='Id')\n",
    "answers_df = pd.read_csv('data/answers.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sententence-transformers to encode the questions\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "questions_df['embedding'] = questions_df.loc[:, 'Body'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    " # save all the embeddings in faiss index\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embeddings = np.array(questions_df['embedding'].tolist())\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the index to a file\n",
    "\n",
    "faiss.write_index(index, 'faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the index from the file\n",
    "\n",
    "index = faiss.read_index('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, n=5, index=index, questions=questions_df, answers=answers_df, model=model):\n",
    "    # Encode the question to get its embedding, ensuring it's in the correct shape\n",
    "    question_embedding = model.encode([question]).reshape(1, -1)\n",
    "    # Search the index for the n closest questions\n",
    "    D, I = index.search(question_embedding, n)\n",
    "    \n",
    "    # Get the IDs of the similar questions\n",
    "    question_ids = questions.iloc[I[0]].index\n",
    "    \n",
    "    # Prepare lists to hold the similar questions and their answers\n",
    "    similar_questions = []\n",
    "    similar_answers = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each similar question ID to get the question text and its answers\n",
    "    for q_id in question_ids:\n",
    "        similar_questions.append(questions.loc[q_id].Body)  # Assuming there's a QuestionText column\n",
    "        q_answers = answers[answers['ParentId'] == q_id]\n",
    "        similar_answers = pd.concat([similar_answers, q_answers], ignore_index=True)\n",
    "    \n",
    "    return question, similar_questions, similar_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question, similar_questions, similar_answers = answer_question('Are are the methods of imputation in statistics?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"Answer to the question shortly by using the information given in these 5 answers. If the information is not there, say that you dont know\"\n",
    "for i, answer in enumerate(similar_answers['Body']): # Assumes a list of answers\n",
    "    content += f\"Start of Answer {i+1}: {answer}\\n End of Answer {i+1}\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=chatgpt_key)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=150,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To handle missing data, you can consider visualizing the missing values in your dataset using libraries like missingno. Depending on the nature of your data and the missingness pattern (MCAR, MAR, MNAR), you can choose between deletion methods (listwise or pairwise deletion), single imputation methods (mean/median/mode substitution, regression imputation, LOCF), or model-based methods (maximum likelihood, multiple imputation). Understanding the reasons for missing data and the context of your dataset can help you decide on the most appropriate imputation technique.'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
